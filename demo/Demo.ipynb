{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a39cd610",
   "metadata": {},
   "source": [
    "# Autoinjector Pose Estimation Demo\n",
    "\n",
    "This notebook demonstrates real-time pose estimation for autoinjectors using a custom-trained YOLO pose model.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The demo pipeline performs the following tasks:\n",
    "1. **Object Detection**: Detects autoinjectors in the frame (capped/uncapped classes)\n",
    "2. **Pose Estimation**: Estimates keypoint locations (front tip and back base)\n",
    "3. **Temporal Smoothing**: Applies EMA filtering for stable keypoint tracking\n",
    "4. **Visualization**: Renders bounding boxes, keypoints, and orientation information\n",
    "\n",
    "## Key Features\n",
    "\n",
    "### Advanced Technical Features\n",
    "- **EMA Smoothing**: Exponential Moving Average filter reduces keypoint jitter for stable tracking\n",
    "- **Model Warmup**: Eliminates first-frame inference latency\n",
    "- **Half-Precision Inference**: FP16 automatically enabled for CUDA devices (2x speedup)\n",
    "- **FPS Throttling**: Adaptive frame rate control for consistent performance\n",
    "- **Orientation Calculation**: Real-time angle and length computation from keypoints\n",
    "\n",
    "### Production-Ready Components\n",
    "- Robust error handling for camera disconnections\n",
    "- Graceful degradation when detections are unavailable\n",
    "- Memory-efficient processing with contiguous array checks\n",
    "- OS-specific camera backend optimizations\n",
    "\n",
    "## Technical Architecture\n",
    "\n",
    "### Class Schema\n",
    "- **Classes**: \n",
    "  - `0` = capped (autoinjector with cap)\n",
    "  - `1` = uncapped (autoinjector without cap)\n",
    "- **Keypoints**:\n",
    "  - `0` = front tip of autoinjector\n",
    "  - `1` = back base of autoinjector\n",
    "\n",
    "### Performance Optimizations\n",
    "1. **Model Warmup**: Pre-initializes CUDA kernels and memory allocation\n",
    "2. **Half-Precision (FP16)**: Automatically enabled for NVIDIA GPUs (requires CUDA compute capability ≥ 7.0)\n",
    "3. **Stream Buffering**: Optional optimization for video file processing\n",
    "4. **Memory Contiguity**: Ensures optimal array layout for OpenCV/PyTorch operations\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. **Configure Model Path**: Update `model_path` to point to your trained model weights\n",
    "2. **Select Device**: Choose appropriate device based on your hardware:\n",
    "   - NVIDIA GPU: `\"cuda\"` or `\"cuda:0\"` (fastest, enables FP16)\n",
    "   - Apple Silicon: `\"mps\"` (note: known pose model issues, use `\"cpu\"` if problems occur)\n",
    "   - CPU: `\"cpu\"` or `None` (slowest but most compatible)\n",
    "3. **Adjust Parameters**: Tune confidence/IoU thresholds based on your use case\n",
    "4. **Run Demo**: Execute the cells to start real-time pose estimation\n",
    "\n",
    "## Known Issues\n",
    "\n",
    "- **Apple MPS Warning**: YOLO pose models have known issues with MPS backend. If you encounter errors, use `device=\"cpu\"` instead.\n",
    "  - Reference: https://github.com/ultralytics/ultralytics/issues/4031\n",
    "\n",
    "## Performance Tips\n",
    "\n",
    "- Lower `max_fps` if experiencing performance issues (reduces CPU/GPU usage)\n",
    "- Use OS-specific camera backends for better stability (see code comments)\n",
    "- Adjust `conf` threshold based on detection requirements (lower = more detections but more false positives)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e26cae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Autoinjector Pose Estimation Demo Notebook\n",
    "\n",
    "This notebook demonstrates real-time pose estimation for autoinjectors using YOLO pose models.\n",
    "The implementation includes advanced features such as:\n",
    "- Exponential Moving Average (EMA) smoothing for stable keypoint tracking\n",
    "- Model warmup to eliminate first-frame inference delays\n",
    "- FPS throttling for consistent performance\n",
    "- Orientation calculation and visualization\n",
    "- Robust error handling for production use\n",
    "\n",
    "Key Technical Decisions:\n",
    "1. EMA smoothing (alpha=0.4) reduces jitter in keypoint detection for stable visualization\n",
    "2. Half-precision (FP16) inference enabled automatically for CUDA devices (performance boost)\n",
    "3. Stream buffer optimization for video processing pipelines\n",
    "4. Adaptive FPS throttling ensures consistent frame timing regardless of inference speed\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "import time\n",
    "\n",
    "# Third-party imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "class AutoInjectorPoseDemo:\n",
    "    \"\"\"\n",
    "    Production-ready YOLO pose estimation demo for autoinjector detection and tracking.\n",
    "    \n",
    "    This class provides an optimized pipeline for real-time pose estimation with advanced\n",
    "    features including temporal smoothing, model warmup, and adaptive frame rate control.\n",
    "    Designed for notebook environments with inline visualization capabilities.\n",
    "    \n",
    "    Class Schema:\n",
    "        - Classes: 0=capped, 1=uncapped (auto-injector state detection)\n",
    "        - Keypoints: 0=front tip, 1=back base (pose estimation)\n",
    "    \n",
    "    Key Features:\n",
    "        - EMA-based keypoint smoothing for stable tracking\n",
    "        - Model warmup to eliminate first-frame latency\n",
    "        - Half-precision inference for CUDA devices (2x speedup)\n",
    "        - Real-time orientation calculation (angle and length)\n",
    "        - Robust error handling and graceful degradation\n",
    "    \n",
    "    Attributes:\n",
    "        KP_FRONT (int): Keypoint index for the front (tip) of the autoinjector.\n",
    "        KP_BACK (int): Keypoint index for the back (base) of the autoinjector.\n",
    "        model (YOLO): Loaded YOLO pose estimation model.\n",
    "        conf (float): Confidence threshold for detections (0.0-1.0).\n",
    "        iou (float): Intersection over Union threshold for NMS (0.0-1.0).\n",
    "        imgsz (int): Input image size for model inference (typically 640 for YOLO).\n",
    "        stream_buffer (bool): Enable stream buffering for video processing optimization.\n",
    "        half (bool): Use FP16 half-precision inference (CUDA only, requires TensorRT/CUDA support).\n",
    "        _kf (dict): Internal state for Exponential Moving Average filtering of keypoints.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keypoint indices as defined in the model's training schema\n",
    "    KP_FRONT = 0  # Front tip of the autoinjector\n",
    "    KP_BACK = 1   # Back base of the autoinjector\n",
    "\n",
    "    def __init__(self, model_path, conf=0.25, iou=0.5, device=None, imgsz=640, stream_buffer=False):\n",
    "        \"\"\"\n",
    "        Initialize the pose estimation demo with optimized configuration.\n",
    "        \n",
    "        Args:\n",
    "            model_path (str): Path to trained YOLO pose model weights (.pt file).\n",
    "            conf (float, optional): Confidence threshold for detections. Lower values\n",
    "                increase recall but may introduce false positives. Defaults to 0.25.\n",
    "            iou (float, optional): IoU threshold for Non-Maximum Suppression.\n",
    "                Higher values allow more overlapping detections. Defaults to 0.5.\n",
    "            device (str, optional): Computation device. Options:\n",
    "                - \"cuda\" or \"cuda:0\" for NVIDIA GPUs (enables FP16 automatically)\n",
    "                - \"mps\" for Apple Silicon Macs (note: MPS has known pose model issues)\n",
    "                - \"cpu\" or None for CPU inference. Defaults to None.\n",
    "            imgsz (int, optional): Input image size. YOLO models are typically trained\n",
    "                at 640x640. Larger sizes improve accuracy but reduce speed. Defaults to 640.\n",
    "            stream_buffer (bool, optional): Enable stream buffering for video processing.\n",
    "                Useful for consistent FPS in video files. Defaults to False.\n",
    "        \n",
    "        Note:\n",
    "            - Model warmup is performed automatically to eliminate first-frame latency\n",
    "            - Half-precision (FP16) is automatically enabled for CUDA devices\n",
    "            - EMA filter state is initialized for keypoint smoothing\n",
    "        \"\"\"\n",
    "        # Load YOLO pose model - task=\"pose\" enables keypoint estimation\n",
    "        self.model = YOLO(model_path, task=\"pose\")\n",
    "        \n",
    "        # Override device if specified (allows forcing CPU/GPU/MPS)\n",
    "        if device:\n",
    "            self.model.overrides.update({\"device\": device})\n",
    "        \n",
    "        # Store inference configuration parameters\n",
    "        self.conf = conf\n",
    "        self.iou = iou\n",
    "        self.imgsz = imgsz\n",
    "        self.stream_buffer = stream_buffer\n",
    "        \n",
    "        # Enable half-precision inference for CUDA devices (2x speedup)\n",
    "        # FP16 requires CUDA compute capability >= 7.0 and proper CUDA/PyTorch setup\n",
    "        self.half = bool(device and device.startswith(\"cuda\"))\n",
    "        \n",
    "        # Initialize EMA filter state for keypoint smoothing\n",
    "        # EMA reduces temporal jitter in keypoint positions for stable visualization\n",
    "        self._kf = {\"f\": None, \"b\": None}  # Front and back keypoint EMA state\n",
    "        \n",
    "        print(\"Loaded model with classes:\", self.model.model.names)\n",
    "\n",
    "        # Model warmup: Run inference on dummy image to eliminate first-frame latency\n",
    "        # This is critical for production systems where first-frame delay is noticeable\n",
    "        # The warmup initializes CUDA kernels, allocates memory, and compiles graph operations\n",
    "        dummy = np.zeros((self.imgsz, self.imgsz, 3), dtype=np.uint8)\n",
    "        _ = self.model.predict(\n",
    "            dummy,\n",
    "            conf=self.conf,\n",
    "            iou=self.iou,\n",
    "            imgsz=self.imgsz,\n",
    "            half=self.half,\n",
    "            stream_buffer=self.stream_buffer,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _put_label(frame, text, org):\n",
    "        \"\"\"\n",
    "        Render text label with black background for optimal readability.\n",
    "        \n",
    "        This utility ensures text is visible regardless of underlying image content,\n",
    "        which is critical for computer vision visualization overlays. Uses anti-aliased\n",
    "        text rendering for professional appearance.\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): BGR image array to draw on (modified in-place).\n",
    "            text (str): Text string to display.\n",
    "            org (tuple): (x, y) coordinates for bottom-left corner of text baseline.\n",
    "        \n",
    "        Note:\n",
    "            Black background rectangle with white text provides high contrast.\n",
    "            Uses OpenCV LINE_AA for smooth, anti-aliased text rendering.\n",
    "        \"\"\"\n",
    "        x, y = org\n",
    "        \n",
    "        # Calculate text dimensions to size background rectangle appropriately\n",
    "        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "        \n",
    "        # Draw solid black background rectangle for text readability\n",
    "        cv2.rectangle(frame, (x - 2, y - th - 4), (x + tw + 2, y + baseline), (0, 0, 0), -1)\n",
    "        \n",
    "        # Draw white text with anti-aliasing (LINE_AA = Line Anti-Aliased)\n",
    "        cv2.putText(frame, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ema(prev, cur, alpha=0.4):\n",
    "        \"\"\"\n",
    "        Exponential Moving Average (EMA) filter for temporal smoothing.\n",
    "        \n",
    "        EMA is a low-pass filter that reduces high-frequency noise while maintaining\n",
    "        responsiveness to actual movements. The alpha parameter controls the balance\n",
    "        between smoothing (low alpha) and responsiveness (high alpha).\n",
    "        \n",
    "        Formula: smoothed = alpha * current + (1 - alpha) * previous\n",
    "        \n",
    "        Args:\n",
    "            prev (np.ndarray or None): Previous smoothed value. None for first frame.\n",
    "            cur (np.ndarray): Current raw value to filter.\n",
    "            alpha (float, optional): Smoothing factor (0.0-1.0). \n",
    "                - Lower values (0.1-0.3): More smoothing, slower response\n",
    "                - Higher values (0.5-0.9): Less smoothing, faster response\n",
    "                Defaults to 0.4 (balanced).\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Smoothed value. Returns cur unchanged if prev is None.\n",
    "        \n",
    "        Note:\n",
    "            Alpha=0.4 provides good balance for keypoint smoothing (40% new value,\n",
    "            60% previous value). This reduces jitter from detection noise while\n",
    "            maintaining reasonable tracking responsiveness.\n",
    "        \"\"\"\n",
    "        # First frame: return current value as-is (no smoothing possible)\n",
    "        if prev is None:\n",
    "            return cur\n",
    "        \n",
    "        # Apply EMA filter: weighted combination of previous and current values\n",
    "        return alpha * cur + (1 - alpha) * prev\n",
    "\n",
    "    def _draw_pose(self, frame, kp: np.ndarray):\n",
    "        \"\"\"\n",
    "        Visualize keypoint pose with temporal smoothing and orientation calculation.\n",
    "        \n",
    "        This method renders the autoinjector pose by:\n",
    "        1. Applying EMA smoothing to reduce keypoint jitter\n",
    "        2. Drawing connecting line between keypoints (if both visible)\n",
    "        3. Rendering keypoint markers with labels\n",
    "        4. Calculating and displaying orientation (angle and length)\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): BGR image array to draw on (modified in-place).\n",
    "            kp (np.ndarray): Keypoint array of shape (N, 2) or (N, 3) where:\n",
    "                - Shape (N, 2): [x, y] coordinates (confidence assumed 1.0)\n",
    "                - Shape (N, 3): [x, y, confidence] coordinates\n",
    "                - N >= 2 required (needs at least front and back keypoints)\n",
    "        \n",
    "        Note:\n",
    "            - Keypoint visibility determined by confidence > 0\n",
    "            - EMA smoothing maintains temporal consistency across frames\n",
    "            - Orientation angle: 0° = horizontal right, 90° = vertical down\n",
    "            - Color scheme: Green line, Red front, Blue back\n",
    "        \"\"\"\n",
    "        # Validate input: ensure keypoints are present and sufficient\n",
    "        if kp is None or kp.size == 0 or kp.shape[0] < 2:\n",
    "            return\n",
    "        \n",
    "        # Handle 2D keypoints (x, y only) by adding confidence channel\n",
    "        # If confidence not provided, assume all keypoints are visible (conf=1.0)\n",
    "        if kp.shape[1] == 2:\n",
    "            kp = np.concatenate([kp, np.ones((kp.shape[0], 1), dtype=kp.dtype)], axis=1)\n",
    "\n",
    "        # Extract front and back keypoint coordinates and confidence scores\n",
    "        xf, yf, cf = kp[self.KP_FRONT]  # Front keypoint: (x, y, confidence)\n",
    "        xb, yb, cb = kp[self.KP_BACK]   # Back keypoint: (x, y, confidence)\n",
    "\n",
    "        # Apply Exponential Moving Average smoothing to reduce temporal jitter\n",
    "        # This creates smooth, stable keypoint tracking even with noisy detections\n",
    "        f_s = self._ema(self._kf[\"f\"], np.array([xf, yf]), 0.4)\n",
    "        b_s = self._ema(self._kf[\"b\"], np.array([xb, yb]), 0.4)\n",
    "        \n",
    "        # Update EMA filter state for next frame\n",
    "        self._kf[\"f\"], self._kf[\"b\"] = f_s, b_s\n",
    "        \n",
    "        # Convert smoothed coordinates to integers for pixel-level drawing\n",
    "        xf, yf = f_s.astype(int)\n",
    "        xb, yb = b_s.astype(int)\n",
    "\n",
    "        # Draw connecting line between keypoints if both are visible\n",
    "        # Green line represents the autoinjector's orientation vector\n",
    "        if cf > 0 and cb > 0:\n",
    "            cv2.line(frame, (xb, yb), (xf, yf), (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw front keypoint (tip) - Red circle with label\n",
    "        if cf > 0:\n",
    "            cv2.circle(frame, (xf, yf), 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, \"front\", (xf + 6, yf - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        \n",
    "        # Draw back keypoint (base) - Blue circle with label\n",
    "        if cb > 0:\n",
    "            cv2.circle(frame, (xb, yb), 5, (255, 0, 0), -1)\n",
    "            cv2.putText(frame, \"back\", (xb + 6, yb - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        # Calculate and display orientation information\n",
    "        # Orientation is computed from back to front (base to tip)\n",
    "        dx, dy = float(xf - xb), float(yf - yb)\n",
    "        \n",
    "        # Calculate angle in degrees: arctan2 handles all quadrants correctly\n",
    "        # 0° = pointing right (horizontal), 90° = pointing down (vertical)\n",
    "        angle = np.degrees(np.arctan2(dy, dx))\n",
    "        \n",
    "        # Calculate length (Euclidean distance) in pixels\n",
    "        length = np.hypot(dx, dy)\n",
    "        \n",
    "        # Display orientation overlay (angle and length) in top-left corner\n",
    "        self._put_label(frame, f\"{angle:.1f} deg, L={length:.0f}px\", (10, 70))\n",
    "\n",
    "    def _process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Process a single frame through the pose estimation pipeline.\n",
    "        \n",
    "        This is the core inference method that:\n",
    "        1. Ensures memory contiguity for optimal OpenCV/PyTorch performance\n",
    "        2. Runs YOLO inference with configured parameters\n",
    "        3. Extracts and validates detection results (boxes, classes, keypoints)\n",
    "        4. Visualizes all detections with bounding boxes, labels, and pose\n",
    "        5. Handles edge cases gracefully with informative overlays\n",
    "        \n",
    "        Args:\n",
    "            frame (np.ndarray): Input BGR image frame (H, W, 3).\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Annotated frame with bounding boxes, labels, keypoints, and\n",
    "                statistics drawn. Original frame returned if inference fails.\n",
    "        \n",
    "        Note:\n",
    "            - Memory contiguity check (ascontiguousarray) is critical for performance\n",
    "            - Early returns handle missing detections gracefully\n",
    "            - Statistics overlay shows detection and keypoint counts for debugging\n",
    "            - All tensor operations moved to CPU and converted to NumPy for OpenCV\n",
    "        \"\"\"\n",
    "        # Ensure memory contiguity for optimal performance\n",
    "        # Non-contiguous arrays can cause significant slowdowns in OpenCV/PyTorch operations\n",
    "        frame = np.ascontiguousarray(frame)\n",
    "        \n",
    "        # Run YOLO inference with configured parameters\n",
    "        # verbose=False suppresses YOLO's default progress logging\n",
    "        results = self.model.predict(\n",
    "            frame,\n",
    "            conf=self.conf,\n",
    "            iou=self.iou,\n",
    "            imgsz=self.imgsz,\n",
    "            half=self.half,\n",
    "            stream_buffer=self.stream_buffer,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Early return if inference produced no results\n",
    "        if not results:\n",
    "            self._put_label(frame, \"no results\", (10, 50))\n",
    "            return frame\n",
    "\n",
    "        # Extract first (and typically only) result from batch\n",
    "        res = results[0]\n",
    "        \n",
    "        # Validate that detections exist (boxes are present)\n",
    "        if res.boxes is None or res.boxes.xyxy is None:\n",
    "            self._put_label(frame, \"no detections\", (10, 50))\n",
    "            return frame\n",
    "\n",
    "        # Extract detection data from PyTorch tensors to NumPy arrays\n",
    "        # All operations moved to CPU for OpenCV compatibility\n",
    "        boxes_np = res.boxes.xyxy.cpu().numpy()  # Bounding boxes: (N, 4) in xyxy format\n",
    "        \n",
    "        # Extract class IDs (integer indices mapping to class names)\n",
    "        clses_np = (\n",
    "            res.boxes.cls.cpu().numpy().astype(int)\n",
    "            if res.boxes.cls is not None\n",
    "            else np.array([], dtype=int)\n",
    "        )\n",
    "        \n",
    "        # Extract confidence scores for each detection\n",
    "        confs_np = (\n",
    "            res.boxes.conf.cpu().numpy()\n",
    "            if res.boxes.conf is not None\n",
    "            else np.array([])\n",
    "        )\n",
    "\n",
    "        # Extract keypoints if available (pose estimation results)\n",
    "        # Check multiple conditions to handle edge cases gracefully\n",
    "        kpts_np = None\n",
    "        if (\n",
    "            hasattr(res, \"keypoints\")\n",
    "            and res.keypoints is not None\n",
    "            and res.keypoints.data is not None\n",
    "            and res.keypoints.data.numel() > 0  # Ensure tensor is non-empty\n",
    "        ):\n",
    "            kpts_np = res.keypoints.data.cpu().numpy()\n",
    "            # Display detection and keypoint statistics for debugging/monitoring\n",
    "            self._put_label(frame, f\"dets {kpts_np.shape[0]}, kpts {kpts_np.shape[1]}\", (10, 45))\n",
    "        else:\n",
    "            # Show detection count even when keypoints are unavailable\n",
    "            self._put_label(frame, f\"dets {boxes_np.shape[0]}, kpts 0\", (10, 45))\n",
    "\n",
    "        # Visualize each detected object\n",
    "        for i in range(boxes_np.shape[0]):\n",
    "            # Extract bounding box coordinates (top-left, bottom-right corners)\n",
    "            x1, y1, x2, y2 = boxes_np[i].astype(int)\n",
    "            \n",
    "            # Draw bounding box (orange color: BGR 0, 200, 255)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 200, 255), 2)\n",
    "\n",
    "            # Build label with class name and confidence score\n",
    "            label = \"object\"  # Default fallback label\n",
    "            if i < clses_np.shape[0]:\n",
    "                cid = clses_np[i]\n",
    "                # Map class ID to human-readable name (e.g., \"capped\", \"uncapped\")\n",
    "                name = self.model.model.names.get(cid, str(cid))\n",
    "                label = name\n",
    "                # Append confidence score if available\n",
    "                if i < confs_np.shape[0]:\n",
    "                    label = f\"{name} {confs_np[i]:.2f}\"\n",
    "            \n",
    "            # Position label above bounding box with minimum y-offset for edge cases\n",
    "            self._put_label(frame, label, (x1, max(20, y1 - 8)))\n",
    "\n",
    "            # Draw keypoint pose visualization if available\n",
    "            if kpts_np is not None and i < kpts_np.shape[0]:\n",
    "                self._draw_pose(frame, kpts_np[i])\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def run_inline(self, camera_index=0, backend=None, max_fps=24, stop_after_seconds=30):\n",
    "        \"\"\"\n",
    "        Run real-time pose estimation demo with webcam capture and inline visualization.\n",
    "        \n",
    "        This method provides a production-ready video capture and processing loop with:\n",
    "        - Configurable camera backend selection (OS-specific optimizations)\n",
    "        - Frame rate throttling for consistent performance\n",
    "        - Adaptive FPS calculation and display\n",
    "        - Robust error handling and graceful shutdown\n",
    "        - Automatic timeout and user interrupt handling\n",
    "        \n",
    "        Args:\n",
    "            camera_index (int, optional): Camera device index (typically 0 for default).\n",
    "                Multiple cameras can be accessed via index 0, 1, 2, etc. Defaults to 0.\n",
    "            backend (int, optional): OpenCV backend for video capture. Examples:\n",
    "                - cv2.CAP_AVFOUNDATION (macOS, recommended for better stability)\n",
    "                - cv2.CAP_DSHOW (Windows)\n",
    "                - cv2.CAP_V4L2 (Linux)\n",
    "                - None (uses default backend). Defaults to None.\n",
    "            max_fps (int, optional): Maximum target frame rate. The system will throttle\n",
    "                processing to maintain this FPS. Lower values reduce CPU/GPU usage.\n",
    "                Set to None for unlimited (uses 30 FPS default). Defaults to 24.\n",
    "            stop_after_seconds (int, optional): Maximum runtime in seconds. Demo will\n",
    "                automatically stop after this duration. Useful for automated testing.\n",
    "                Set to a large value for indefinite operation. Defaults to 30.\n",
    "        \n",
    "        Note:\n",
    "            - Camera resolution set to 1280x720 for balance of quality and performance\n",
    "            - MJPG codec used for efficient video capture\n",
    "            - FPS throttling ensures consistent performance regardless of inference speed\n",
    "            - Fail-safe mechanism handles camera disconnection gracefully\n",
    "            - Press 'q' key or Ctrl+C to stop early\n",
    "        \n",
    "        Raises:\n",
    "            KeyboardInterrupt: Handled gracefully, allows cleanup before exit.\n",
    "        \"\"\"\n",
    "        # Initialize video capture with optional backend specification\n",
    "        # Backend specification is important for OS-specific camera access optimizations\n",
    "        cap = (\n",
    "            cv2.VideoCapture(camera_index)\n",
    "            if backend is None\n",
    "            else cv2.VideoCapture(camera_index, backend)\n",
    "        )\n",
    "        \n",
    "        # Validate camera access\n",
    "        if not cap.isOpened():\n",
    "            print(\"Cannot open webcam at index:\", camera_index, \"| backend:\", backend)\n",
    "            return\n",
    "\n",
    "        # Configure camera settings for optimal performance\n",
    "        # MJPG codec provides good compression and is widely supported\n",
    "        cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*\"MJPG\"))\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)   # 720p width\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)   # 720p height\n",
    "\n",
    "        # Calculate FPS throttling parameters\n",
    "        target_fps = max_fps or 30  # Default to 30 FPS if not specified\n",
    "        wait_time = max(1, int(1000 / target_fps))  # OpenCV waitKey time in milliseconds\n",
    "\n",
    "        # Create resizable window for flexible viewing\n",
    "        # WINDOW_NORMAL allows manual resizing by user\n",
    "        cv2.namedWindow(\"Autoinjector Pose Demo\", cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(\"Autoinjector Pose Demo\", 960, 540)  # Initial window size\n",
    "\n",
    "        # Initialize timing and error tracking variables\n",
    "        t_start = time.time()  # Start time for timeout calculation\n",
    "        prev = 0.0  # Previous frame time for FPS calculation\n",
    "        fail_count, max_fail = 0, 30  # Camera read failure counter (robustness)\n",
    "\n",
    "        print(f\"Running for up to {stop_after_seconds} seconds. Press 'q' to stop early.\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                frame_start = time.time()  # Track frame processing start time\n",
    "                \n",
    "                # Read frame from camera\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                # Handle camera read failures gracefully\n",
    "                # This prevents crashes from temporary camera disconnections\n",
    "                if not ret or frame is None:\n",
    "                    fail_count += 1\n",
    "                    if fail_count >= max_fail:\n",
    "                        print(\"Camera feed lost\")\n",
    "                        break\n",
    "                    continue\n",
    "                fail_count = 0  # Reset counter on successful read\n",
    "\n",
    "                # Process frame through pose estimation pipeline\n",
    "                frame = self._process_frame(frame)\n",
    "\n",
    "                # Calculate and display FPS (frames per second)\n",
    "                # Uses wall-clock time for accurate real-time measurement\n",
    "                now = time.time()\n",
    "                fps_disp = 1.0 / (now - prev) if prev > 0 else 0.0\n",
    "                prev = now\n",
    "                self._put_label(frame, f\"FPS {fps_disp:.1f}\", (10, 25))\n",
    "\n",
    "                # Display annotated frame\n",
    "                cv2.imshow(\"Autoinjector Pose Demo\", frame)\n",
    "\n",
    "                # Check for user exit command (q key)\n",
    "                if cv2.waitKey(wait_time) & 0xFF == ord(\"q\"):\n",
    "                    print(\"Stopped by user (pressed 'q').\")\n",
    "                    break\n",
    "\n",
    "                # Check for timeout\n",
    "                if time.time() - t_start > stop_after_seconds:\n",
    "                    print(f\"Reached time limit of {stop_after_seconds} seconds.\")\n",
    "                    break\n",
    "\n",
    "                # FPS throttling: maintain consistent frame rate\n",
    "                # This ensures predictable performance and prevents excessive CPU/GPU usage\n",
    "                elapsed = time.time() - frame_start\n",
    "                budget = 1.0 / target_fps  # Time budget per frame\n",
    "                if elapsed < budget:\n",
    "                    # Sleep if we finished processing early to maintain target FPS\n",
    "                    time.sleep(budget - elapsed)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            # Handle Ctrl+C gracefully\n",
    "            print(\"Stopped by keyboard interrupt.\")\n",
    "        finally:\n",
    "            # Cleanup: always release resources regardless of exit condition\n",
    "            cap.release()\n",
    "            cv2.destroyWindow(\"Autoinjector Pose Demo\")\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76f0cb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model with classes: {0: 'capped', 1: 'uncapped'}\n",
      "WARNING ⚠️ Apple MPS known Pose bug. Recommend 'device=cpu' for Pose models. See https://github.com/ultralytics/ultralytics/issues/4031.\n",
      "Running for up to 30 seconds. Press 'q' to stop early.\n",
      "Reached time limit of 30 seconds.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demo Execution Cell\n",
    "\n",
    "This cell demonstrates how to use the AutoInjectorPoseDemo class for real-time\n",
    "pose estimation with a webcam. Configure the parameters below based on your setup.\n",
    "\"\"\"\n",
    "\n",
    "# Configure model path (relative to notebook location)\n",
    "model_path = \"../model_training/training_runs/example_results/weights/last.pt\"\n",
    "\n",
    "# Device selection for inference\n",
    "# Options:\n",
    "#   - \"cuda\" or \"cuda:0\" for NVIDIA GPUs (enables FP16, fastest)\n",
    "#   - \"mps\" for Apple Silicon Macs (note: MPS has known pose model issues)\n",
    "#   - \"cpu\" or None for CPU inference (slowest but most compatible)\n",
    "# \n",
    "# Recommendation: Use \"cpu\" for MPS devices until Ultralytics fixes the pose bug\n",
    "# See: https://github.com/ultralytics/ultralytics/issues/4031\n",
    "device_choice = \"mps\"  # Change to \"cpu\" if you encounter MPS warnings\n",
    "\n",
    "# Initialize the pose estimation demo\n",
    "# Parameters:\n",
    "#   - conf: Confidence threshold (0.25 = balanced, lower = more detections)\n",
    "#   - iou: IoU threshold for NMS (0.5 = standard, higher = more overlapping boxes)\n",
    "#   - device: Computation device (see options above)\n",
    "#   - imgsz: Input image size (640 = standard YOLO size, larger = slower but more accurate)\n",
    "#   - stream_buffer: Enable for video files (False for webcam)\n",
    "demo = AutoInjectorPoseDemo(\n",
    "    model_path,\n",
    "    conf=0.25,      # Confidence threshold\n",
    "    iou=0.5,        # IoU threshold for NMS\n",
    "    device=device_choice,\n",
    "    imgsz=640,      # Input image size\n",
    "    stream_buffer=False  # Disable for webcam (enable for video files)\n",
    ")\n",
    "\n",
    "# Run the demo with webcam capture\n",
    "# Parameters:\n",
    "#   - camera_index: Camera device index (0 = default, 1 = second camera, etc.)\n",
    "#   - backend: OS-specific camera backend (None = auto-detect)\n",
    "#              macOS: cv2.CAP_AVFOUNDATION (recommended for stability)\n",
    "#              Windows: cv2.CAP_DSHOW\n",
    "#              Linux: cv2.CAP_V4L2\n",
    "#   - max_fps: Target frame rate (24 = smooth, lower = less CPU/GPU usage)\n",
    "#   - stop_after_seconds: Auto-stop after N seconds (30 = demo mode)\n",
    "#\n",
    "# Usage tips:\n",
    "#   - If preview stalls, try specifying the OS-specific backend\n",
    "#   - Lower max_fps if you experience performance issues\n",
    "#   - Press 'q' to stop early\n",
    "demo.run_inline(\n",
    "    camera_index=0,          # Default camera\n",
    "    backend=None,            # Auto-detect backend (try cv2.CAP_AVFOUNDATION on macOS if issues)\n",
    "    max_fps=24,             # Target 24 FPS (smooth real-time performance)\n",
    "    stop_after_seconds=30   # Run for 30 seconds (increase for longer demos)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9073be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
